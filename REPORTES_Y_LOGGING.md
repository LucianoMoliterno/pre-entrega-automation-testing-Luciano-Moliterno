# ğŸ“Š GeneraciÃ³n de Reportes y Logging

## DocumentaciÃ³n del Sistema de Reportes
**Autor**: Luciano Moliterno  
**Proyecto**: Automation Testing Framework - SauceDemo UI + JSONPlaceholder API

---

## ğŸ“‹ Ãndice
1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [TecnologÃ­as Implementadas](#tecnologÃ­as-implementadas)
3. [Estructura de Carpetas](#estructura-de-carpetas)
4. [ConfiguraciÃ³n de Reportes](#configuraciÃ³n-de-reportes)
5. [Sistema de Logging](#sistema-de-logging)
6. [Captura de Screenshots](#captura-de-screenshots)
7. [Reintentos AutomÃ¡ticos](#reintentos-automÃ¡ticos)
8. [CÃ³mo Ejecutar y Generar Reportes](#cÃ³mo-ejecutar-y-generar-reportes)
9. [InterpretaciÃ³n de Reportes](#interpretaciÃ³n-de-reportes)

---

## ğŸ“– DescripciÃ³n General

Este framework de automatizaciÃ³n incluye un **sistema completo de generaciÃ³n de reportes y logging** que proporciona evidencia legible y profesional para el cliente. El sistema estÃ¡ integrado tanto con las pruebas UI de **SauceDemo** como con las pruebas API de **JSONPlaceholder**.

### CaracterÃ­sticas Principales:
âœ… Reportes HTML interactivos con `pytest-html`  
âœ… Logging detallado en archivos y consola  
âœ… Captura automÃ¡tica de screenshots en fallos  
âœ… Reintentos automÃ¡ticos con `pytest-rerunfailures`  
âœ… Metadata personalizada del autor y proyecto  
âœ… ClasificaciÃ³n automÃ¡tica de tests (UI/API/E2E)  
âœ… Timestamps y seguimiento temporal  

---

## ğŸ› ï¸ TecnologÃ­as Implementadas

### Dependencias Instaladas:
```
pytest==7.4.0
pytest-html==3.2.0
pytest-rerunfailures==12.0
selenium==4.15.1
requests==2.31.0
faker==20.0.0
```

### LibrerÃ­as Utilizadas:
- **pytest-html**: GeneraciÃ³n de reportes HTML profesionales
- **pytest-rerunfailures**: Reintentos automÃ¡ticos para tests flaky
- **logging**: Sistema de logging nativo de Python
- **pathlib**: GestiÃ³n de rutas y archivos

---

## ğŸ“ Estructura de Carpetas

```
pre-entrega-automation-testing-Luciano-Moliterno/
â”‚
â”œâ”€â”€ reports/                          # Reportes HTML generados
â”‚   â”œâ”€â”€ .gitkeep                      # Asegura versionado en Git
â”‚   â””â”€â”€ reporte_completo.html         # Reporte principal (autogenerado)
â”‚
â”œâ”€â”€ logs/                             # Archivos de logging
â”‚   â”œâ”€â”€ .gitkeep                      # Asegura versionado en Git
â”‚   â”œâ”€â”€ pytest_execution.log          # Log principal (autogenerado)
â”‚   â””â”€â”€ test_execution_YYYYMMDD_HHMMSS.log  # Logs timestamped
â”‚
â”œâ”€â”€ screenshots/                      # Capturas de pantalla
â”‚   â””â”€â”€ test_name_YYYYMMDD_HHMMSS.png # Screenshots de fallos
â”‚
â”œâ”€â”€ conftest.py                       # Hooks y fixtures de pytest
â”œâ”€â”€ pytest.ini                        # ConfiguraciÃ³n de pytest
â””â”€â”€ requirements.txt                  # Dependencias del proyecto
```

### âš ï¸ Nota sobre Versionado
Las carpetas `reports/` y `logs/` contienen archivos `.gitkeep` para asegurar que se versionan en Git aunque estÃ©n vacÃ­as. Los archivos generados (logs y reportes) se excluyen tÃ­picamente del control de versiones mediante `.gitignore`.

---

## âš™ï¸ ConfiguraciÃ³n de Reportes

### pytest.ini - ConfiguraciÃ³n Principal

```ini
[pytest]
# Autor: Luciano Moliterno
# Proyecto: SauceDemo UI + JSONPlaceholder API Testing

addopts = 
    -v                                    # Verbose
    --strict-markers                      # ValidaciÃ³n estricta de marcadores
    --tb=short                            # Traceback corto
    --html=reports/reporte_completo.html  # UbicaciÃ³n del reporte
    --self-contained-html                 # Reporte autocontenido
    --reruns 2                            # 2 reintentos en caso de fallo
    --reruns-delay 1                      # 1 segundo entre reintentos
```

### conftest.py - Hooks Implementados

#### 1. **pytest_configure()**
- Configura marcadores personalizados (smoke, api, ui, e2e)
- Agrega metadata del autor y proyecto al reporte
- Inicializa el sistema de logging

#### 2. **pytest_html_report_title()**
- Personaliza el tÃ­tulo del reporte HTML
- Muestra: "Reporte de Pruebas Automatizadas - Luciano Moliterno"

#### 3. **pytest_html_results_summary()**
- Agrega informaciÃ³n del proyecto y autor al resumen
- Visible en la parte superior del reporte

#### 4. **pytest_html_results_table_header()**
- Personaliza columnas de la tabla de resultados
- Columnas: Test, Time, Description, Test Type, Result

#### 5. **pytest_html_results_table_row()**
- Clasifica automÃ¡ticamente tests como UI/API/E2E
- Agrega timestamps a cada test ejecutado

#### 6. **pytest_runtest_makereport()**
- Hook principal para captura de evidencia
- Registra logs detallados de cada test
- Captura screenshots automÃ¡ticamente en fallos UI
- Agrega screenshots al reporte HTML

#### 7. **pytest_sessionstart() / pytest_sessionfinish()**
- Registra inicio y fin de la sesiÃ³n de pruebas
- Genera resumen de ejecuciÃ³n

---

## ğŸ“ Sistema de Logging

### ConfiguraciÃ³n Multi-Handler

El sistema de logging escribe simultÃ¡neamente en:
1. **Archivo timestamped**: `logs/test_execution_YYYYMMDD_HHMMSS.log`
2. **Archivo principal**: `logs/pytest_execution.log` (configurado en pytest.ini)
3. **Consola**: Salida en tiempo real durante ejecuciÃ³n

### Niveles de Logging

```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / f"test_execution_{timestamp}.log"),
        logging.StreamHandler()
    ]
)
```

### Tipos de Mensajes

- **INFO**: EjecuciÃ³n normal, inicio/fin de tests
- **ERROR**: Fallos en tests, errores de ejecuciÃ³n
- **WARNING**: Tests skipped, advertencias

### Ejemplo de Log

```
2025-10-16 14:30:15 [INFO] __main__ - INICIANDO EJECUCIÃ“N DE PRUEBAS
2025-10-16 14:30:15 [INFO] __main__ - Autor: Luciano Moliterno
2025-10-16 14:30:20 [INFO] __main__ - Iniciando WebDriver para test UI
2025-10-16 14:30:25 [INFO] __main__ - âœ“ PASSED: tests/test_login.py::test_login_exitoso
2025-10-16 14:30:30 [ERROR] __main__ - âœ— FAILED: tests/test_carrito.py::test_agregar_producto
2025-10-16 14:30:30 [INFO] __main__ - Screenshot capturado: screenshots/test_agregar_producto_20251016_143030.png
```

---

## ğŸ“¸ Captura de Screenshots

### Captura AutomÃ¡tica en Fallos

El sistema captura screenshots automÃ¡ticamente cuando:
- Un test UI **falla** durante la fase de ejecuciÃ³n
- El test tiene acceso al fixture `driver`
- El WebDriver estÃ¡ activo

### Proceso de Captura

```python
if report.failed and report.when == "call":
    if "driver" in item.funcargs:
        screenshot_path = f"screenshots/{test_name}_{timestamp}.png"
        driver.save_screenshot(screenshot_path)
        # Agregar al reporte HTML
        report.extra.append(pytest_html.extras.image(screenshot_path))
```

### Nomenclatura de Screenshots

```
test_login_exitoso_20251016_143025.png
test_agregar_producto_carrito_20251016_143045.png
test_verificar_contenido_carrito_20251016_143102.png
```

Formato: `{nombre_test}_{YYYYMMDD}_{HHMMSS}.png`

---

## ğŸ”„ Reintentos AutomÃ¡ticos

### ConfiguraciÃ³n de pytest-rerunfailures

```ini
--reruns 2          # MÃ¡ximo 2 reintentos por test
--reruns-delay 1    # 1 segundo de espera entre reintentos
```

### Funcionamiento

1. Test falla en el primer intento
2. Sistema espera 1 segundo
3. Reintenta el test (hasta 2 veces)
4. Si pasa en algÃºn reintento â†’ âœ… PASSED
5. Si falla todos los intentos â†’ âŒ FAILED

### Casos de Uso

- Tests flaky por timing issues
- Problemas transitorios de red (API tests)
- Carga lenta de elementos UI

### Deshabilitar Reintentos en Test EspecÃ­fico

```python
@pytest.mark.no_rerun
def test_sin_reintentos():
    """Este test no se reintentarÃ¡ si falla"""
    pass
```

---

## ğŸš€ CÃ³mo Ejecutar y Generar Reportes

### Ejecutar Todos los Tests

```bash
pytest
```
Genera: `reports/reporte_completo.html`

### Ejecutar Solo Tests UI

```bash
pytest tests/ -v
```

### Ejecutar Solo Tests API

```bash
pytest test_api/ -v
```

### Ejecutar Tests E2E

```bash
pytest -m e2e -v
```

### Ejecutar Tests Smoke

```bash
pytest -m smoke -v
```

### Ejecutar con Reporte Personalizado

```bash
pytest --html=reports/reporte_custom.html --self-contained-html
```

### Ejecutar sin Reintentos

```bash
pytest --reruns 0
```

### Ejecutar con MÃ¡s Reintentos

```bash
pytest --reruns 5 --reruns-delay 2
```

---

## ğŸ“Š InterpretaciÃ³n de Reportes

### Estructura del Reporte HTML

El reporte generado incluye:

#### 1. **Encabezado**
- TÃ­tulo: "Reporte de Pruebas Automatizadas - Luciano Moliterno"
- Fecha y hora de ejecuciÃ³n
- Metadata del proyecto y autor

#### 2. **Resumen Ejecutivo**
```
Proyecto: Automation Testing Framework
Autor: Luciano Moliterno - QA Automation Engineer
Ambiente: Testing
Python: 3.11.2
Pytest: 7.4.0
```

#### 3. **EstadÃ­sticas Generales**
- Total de tests ejecutados
- Tests pasados âœ…
- Tests fallidos âŒ
- Tests omitidos âŠ˜
- Tiempo total de ejecuciÃ³n

#### 4. **Tabla de Resultados**

| Test | Time | Description | Test Type | Result |
|------|------|-------------|-----------|--------|
| test_login_exitoso | 14:30:25 | Prueba login exitoso | UI | âœ… PASSED |
| test_post_lifecycle | 14:31:10 | Test E2E ciclo completo | E2E | âœ… PASSED |
| test_get_users | 14:31:45 | Obtiene usuarios | API | âœ… PASSED |

#### 5. **Detalles de Fallos**
- Stack trace completo
- Mensaje de error
- Screenshot (si aplica)
- Logs relacionados

#### 6. **Screenshots Embebidos**
Los screenshots de fallos se incrustan directamente en el reporte HTML para fÃ¡cil visualizaciÃ³n.

---

## ğŸ¯ IntegraciÃ³n con el Proyecto Final

### Tests UI (SauceDemo)

âœ… Login con diferentes usuarios  
âœ… NavegaciÃ³n del catÃ¡logo  
âœ… GestiÃ³n del carrito de compras  
âœ… ValidaciÃ³n de productos  
âœ… Captura de screenshots en fallos  

### Tests API (JSONPlaceholder)

âœ… Operaciones CRUD (POST, PATCH, DELETE)  
âœ… ValidaciÃ³n de esquemas JSON  
âœ… ValidaciÃ³n de tipos de datos  
âœ… MediciÃ³n de tiempos de respuesta  
âœ… Tests parametrizados  

### Tests E2E

âœ… Ciclo de vida completo de posts  
âœ… Datos generados con Faker  
âœ… Validaciones exhaustivas  
âœ… Logging detallado de cada paso  

---

## ğŸ“ˆ MÃ©tricas y KPIs

El sistema de reportes proporciona:

### MÃ©tricas de EjecuciÃ³n
- Tiempo promedio por test
- Tasa de Ã©xito/fallo
- Tests mÃ¡s lentos
- Tests con reintentos

### MÃ©tricas de Calidad
- Cobertura de funcionalidades
- Estabilidad de tests (flakiness)
- DistribuciÃ³n UI vs API
- Tests crÃ­ticos (smoke)

---

## ğŸ”§ Troubleshooting

### Problema: No se genera el reporte HTML
**SoluciÃ³n**: Verificar que pytest-html estÃ© instalado
```bash
pip install pytest-html==3.2.0
```

### Problema: No se capturan screenshots
**SoluciÃ³n**: Verificar permisos de escritura en carpeta `screenshots/`

### Problema: Logs vacÃ­os
**SoluciÃ³n**: Verificar configuraciÃ³n de logging en pytest.ini

### Problema: Reintentos no funcionan
**SoluciÃ³n**: Verificar que pytest-rerunfailures estÃ© instalado
```bash
pip install pytest-rerunfailures==12.0
```

---

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n pytest-html](https://pytest-html.readthedocs.io/)
- [DocumentaciÃ³n pytest-rerunfailures](https://github.com/pytest-dev/pytest-rerunfailures)
- [Pytest Logging](https://docs.pytest.org/en/stable/logging.html)
- [Selenium Screenshots](https://selenium-python.readthedocs.io/api.html#selenium.webdriver.remote.webdriver.WebDriver.save_screenshot)

---

## âœ… Checklist de Entrega Final

- [x] pytest-html instalado y configurado
- [x] pytest-rerunfailures instalado y configurado
- [x] requirements.txt actualizado
- [x] Carpetas reports/ y logs/ versionadas con .gitkeep
- [x] Hooks de captura implementados en conftest.py
- [x] pytest.ini con tÃ­tulo y metadata de autor
- [x] Sistema de logging completo
- [x] Captura automÃ¡tica de screenshots
- [x] IntegraciÃ³n con tests UI de SauceDemo
- [x] IntegraciÃ³n con tests API de JSONPlaceholder
- [x] DocumentaciÃ³n completa

---

## ğŸ‘¨â€ğŸ’» Autor

**Luciano Moliterno**  
QA Automation Engineer  
Proyecto: Automation Testing Framework  
Fecha: Octubre 2025

---

**Este sistema de reportes cumple con todos los requisitos de la entrega final y proporciona evidencia profesional y legible para el cliente.**

